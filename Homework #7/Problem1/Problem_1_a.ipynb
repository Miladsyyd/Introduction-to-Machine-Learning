{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cp2aLAG7QYeb"
      },
      "outputs": [],
      "source": [
        "# Mohammadmilad Sayyad\n",
        "# Problem 1.a\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import time\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Hyperparameters (you can tune batch_size or lr if needed)\n",
        "num_epochs = 300          # <<< For testing, set to 5 or 10 first\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "\n",
        "# CIFAR-10 normalization values (standard)\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "# Data transforms: basic preprocessing (no heavy augmentation for baseline)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n"
      ],
      "metadata": {
        "id": "cdfg-EPmQou-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc60f3a9-45ea-4967-e91a-d43e272966bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download CIFAR-10 (train and test)\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgmoMcvORMNk",
        "outputId": "ecfb2602-e8e1-41ec-d914-254813e79ea9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 50000\n",
            "Test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Baseline(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN_Baseline, self).__init__()\n",
        "        # Convolutional part\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # After two 2x2 pools on 32x32:\n",
        "        # 32x32 -> 16x16 -> 8x8\n",
        "        # Channels: 64\n",
        "        # Flatten size = 64 * 8 * 8 = 4096\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (N, 3, 32, 32)\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> (N, 32, 16, 16)\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> (N, 64, 8, 8)\n",
        "        x = x.view(x.size(0), -1)             # -> (N, 4096)\n",
        "        x = F.relu(self.fc1(x))               # -> (N, 256)\n",
        "        x = self.fc2(x)                       # -> (N, 10)\n",
        "        return x\n",
        "\n",
        "# Instantiate model\n",
        "model = CNN_Baseline(num_classes=10).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R4uwUWeRM1V",
        "outputId": "21183a14-8b75-4b36-e4d8-5b691a0b79b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN_Baseline(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4096, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, criterion, dataloader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader.dataset)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "WpbvJheuRSDs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Problem 1.a – Training (50 epochs) =====\n",
        "\n",
        "# Utility function placed here so it's always defined\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# (Re)create the model so this run is fresh\n",
        "model = CNN_Baseline(num_classes=10).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# How many epochs for this experiment\n",
        "num_epochs = 50\n",
        "\n",
        "# Model size (number of parameters)\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of trainable parameters (model size): {num_params}\")\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Starting training for Problem 1.a (Baseline CNN) with 50 epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_train_loss = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f} | \"\n",
        "          f\"Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time_seconds = end_time - start_time\n",
        "\n",
        "print(\"\\n=== Final Results for Problem 1.a (Baseline CNN, 50 epochs) ===\")\n",
        "print(f\"Total training time: {training_time_seconds:.2f} seconds\")\n",
        "print(f\"Final training loss (epoch {num_epochs}): {train_losses[-1]:.4f}\")\n",
        "print(f\"Final test loss (epoch {num_epochs}): {test_losses[-1]:.4f}\")\n",
        "print(f\"Final test accuracy (epoch {num_epochs}): {test_accuracies[-1]:.2f}%\")\n",
        "print(f\"Model size (number of parameters): {num_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlQdWVT8bAa_",
        "outputId": "32bb11b2-9312-4d13-9bb9-f188d420f269"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters (model size): 1070794\n",
            "Starting training for Problem 1.a (Baseline CNN) with 50 epochs...\n",
            "Epoch [1/50] Train Loss: 1.5300 | Test Loss: 1.2430 | Test Acc: 55.36%\n",
            "Epoch [2/50] Train Loss: 1.0903 | Test Loss: 0.9936 | Test Acc: 65.06%\n",
            "Epoch [3/50] Train Loss: 0.8903 | Test Loss: 0.9422 | Test Acc: 66.90%\n",
            "Epoch [4/50] Train Loss: 0.7553 | Test Loss: 0.8518 | Test Acc: 70.54%\n",
            "Epoch [5/50] Train Loss: 0.6384 | Test Loss: 0.8936 | Test Acc: 69.95%\n",
            "Epoch [6/50] Train Loss: 0.5145 | Test Loss: 0.8371 | Test Acc: 72.48%\n",
            "Epoch [7/50] Train Loss: 0.4014 | Test Loss: 0.8509 | Test Acc: 72.96%\n",
            "Epoch [8/50] Train Loss: 0.2994 | Test Loss: 0.9226 | Test Acc: 73.45%\n",
            "Epoch [9/50] Train Loss: 0.2118 | Test Loss: 1.0373 | Test Acc: 72.29%\n",
            "Epoch [10/50] Train Loss: 0.1379 | Test Loss: 1.1548 | Test Acc: 72.39%\n",
            "Epoch [11/50] Train Loss: 0.0979 | Test Loss: 1.2979 | Test Acc: 72.18%\n",
            "Epoch [12/50] Train Loss: 0.0673 | Test Loss: 1.4139 | Test Acc: 71.81%\n",
            "Epoch [13/50] Train Loss: 0.0492 | Test Loss: 1.4426 | Test Acc: 73.07%\n",
            "Epoch [14/50] Train Loss: 0.0421 | Test Loss: 1.5896 | Test Acc: 72.51%\n",
            "Epoch [15/50] Train Loss: 0.0346 | Test Loss: 1.6060 | Test Acc: 73.03%\n",
            "Epoch [16/50] Train Loss: 0.0268 | Test Loss: 1.6374 | Test Acc: 73.51%\n",
            "Epoch [17/50] Train Loss: 0.0075 | Test Loss: 1.7713 | Test Acc: 72.68%\n",
            "Epoch [18/50] Train Loss: 0.0039 | Test Loss: 1.7500 | Test Acc: 74.46%\n",
            "Epoch [19/50] Train Loss: 0.0019 | Test Loss: 1.7880 | Test Acc: 74.36%\n",
            "Epoch [20/50] Train Loss: 0.0034 | Test Loss: 1.8074 | Test Acc: 74.13%\n",
            "Epoch [21/50] Train Loss: 0.0018 | Test Loss: 1.8124 | Test Acc: 74.39%\n",
            "Epoch [22/50] Train Loss: 0.0005 | Test Loss: 1.8710 | Test Acc: 74.49%\n",
            "Epoch [23/50] Train Loss: 0.0004 | Test Loss: 1.9065 | Test Acc: 74.48%\n",
            "Epoch [24/50] Train Loss: 0.0003 | Test Loss: 1.9308 | Test Acc: 74.49%\n",
            "Epoch [25/50] Train Loss: 0.0003 | Test Loss: 1.9533 | Test Acc: 74.49%\n",
            "Epoch [26/50] Train Loss: 0.0002 | Test Loss: 1.9731 | Test Acc: 74.54%\n",
            "Epoch [27/50] Train Loss: 0.0002 | Test Loss: 1.9887 | Test Acc: 74.54%\n",
            "Epoch [28/50] Train Loss: 0.0002 | Test Loss: 2.0036 | Test Acc: 74.53%\n",
            "Epoch [29/50] Train Loss: 0.0002 | Test Loss: 2.0158 | Test Acc: 74.56%\n",
            "Epoch [30/50] Train Loss: 0.0002 | Test Loss: 2.0288 | Test Acc: 74.55%\n",
            "Epoch [31/50] Train Loss: 0.0002 | Test Loss: 2.0399 | Test Acc: 74.56%\n",
            "Epoch [32/50] Train Loss: 0.0002 | Test Loss: 2.0505 | Test Acc: 74.54%\n",
            "Epoch [33/50] Train Loss: 0.0002 | Test Loss: 2.0609 | Test Acc: 74.53%\n",
            "Epoch [34/50] Train Loss: 0.0001 | Test Loss: 2.0703 | Test Acc: 74.59%\n",
            "Epoch [35/50] Train Loss: 0.0001 | Test Loss: 2.0783 | Test Acc: 74.56%\n",
            "Epoch [36/50] Train Loss: 0.0001 | Test Loss: 2.0862 | Test Acc: 74.56%\n",
            "Epoch [37/50] Train Loss: 0.0001 | Test Loss: 2.0947 | Test Acc: 74.64%\n",
            "Epoch [38/50] Train Loss: 0.0001 | Test Loss: 2.1019 | Test Acc: 74.54%\n",
            "Epoch [39/50] Train Loss: 0.0001 | Test Loss: 2.1092 | Test Acc: 74.56%\n",
            "Epoch [40/50] Train Loss: 0.0001 | Test Loss: 2.1161 | Test Acc: 74.55%\n",
            "Epoch [41/50] Train Loss: 0.0001 | Test Loss: 2.1227 | Test Acc: 74.62%\n",
            "Epoch [42/50] Train Loss: 0.0001 | Test Loss: 2.1293 | Test Acc: 74.61%\n",
            "Epoch [43/50] Train Loss: 0.0001 | Test Loss: 2.1352 | Test Acc: 74.63%\n",
            "Epoch [44/50] Train Loss: 0.0001 | Test Loss: 2.1402 | Test Acc: 74.63%\n",
            "Epoch [45/50] Train Loss: 0.0001 | Test Loss: 2.1473 | Test Acc: 74.54%\n",
            "Epoch [46/50] Train Loss: 0.0001 | Test Loss: 2.1524 | Test Acc: 74.64%\n",
            "Epoch [47/50] Train Loss: 0.0001 | Test Loss: 2.1573 | Test Acc: 74.63%\n",
            "Epoch [48/50] Train Loss: 0.0001 | Test Loss: 2.1626 | Test Acc: 74.62%\n",
            "Epoch [49/50] Train Loss: 0.0001 | Test Loss: 2.1676 | Test Acc: 74.56%\n",
            "Epoch [50/50] Train Loss: 0.0001 | Test Loss: 2.1723 | Test Acc: 74.60%\n",
            "\n",
            "=== Final Results for Problem 1.a (Baseline CNN, 50 epochs) ===\n",
            "Total training time: 382.40 seconds\n",
            "Final training loss (epoch 50): 0.0001\n",
            "Final test loss (epoch 50): 2.1723\n",
            "Final test accuracy (epoch 50): 74.60%\n",
            "Model size (number of parameters): 1070794\n"
          ]
        }
      ]
    }
  ]
}