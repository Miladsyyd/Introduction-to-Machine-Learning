{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ELvE-Ot7eQpP"
      },
      "outputs": [],
      "source": [
        "# Mohammadmilad Sayyad\n",
        "# Problem 1.b\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "num_epochs = 100   # <<< PROBLEM 1.b requirement\n",
        "\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std  = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hr4Rvvy_efHf",
        "outputId": "5416a88b-975d-4787-9240-a6d799c2421b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FMEqPJcZefn3",
        "outputId": "86d38b18-3380-4277-d789-83e1feb6a3ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Deeper(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN_Deeper, self).__init__()\n",
        "\n",
        "        # Block 1: 3 → 32\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "\n",
        "        # Block 2: 32 → 64\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Block 3: 64 → 128  (NEW LAYER)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # After 3 poolings: 32→16→8→4\n",
        "        # Feature size = 128 * 4 * 4 = 2048\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))   # -> (32, 16, 16)\n",
        "        x = self.pool(F.relu(self.conv2(x)))   # -> (64, 8,  8)\n",
        "        x = self.pool(F.relu(self.conv3(x)))   # -> (128,4,  4)\n",
        "\n",
        "        x = x.view(x.size(0), -1)              # Flatten → 2048\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN_Deeper().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-dESAAP0ehY4",
        "outputId": "633da432-4bfd-47f9-9817-3c65ef44fac0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN_Deeper(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_params = count_parameters(model)\n",
        "print(\"Model size (trainable parameters):\", num_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0Qy5DZfoemV3",
        "outputId": "a43b26a0-43ff-476f-98ef-ef62fe486338"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size (trainable parameters): 620362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, criterion, dataloader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / len(dataloader.dataset), 100 * correct / total\n"
      ],
      "metadata": {
        "id": "oTXY_w6RepUP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Training CNN_Deeper (Problem 1.b) for 100 epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f} | \"\n",
        "          f\"Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(\"\\n=== Final Results for Problem 1.b (Deeper CNN, 100 epochs) ===\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n",
        "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
        "print(f\"Total Parameters: {num_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7H-Xi1fPeqRn",
        "outputId": "cbb19af0-f046-49b9-8c00-5729d2acec92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN_Deeper (Problem 1.b) for 100 epochs...\n",
            "Epoch [1/100] Train Loss: 1.7106 | Test Loss: 1.2963 | Test Acc: 53.59%\n",
            "Epoch [2/100] Train Loss: 1.1841 | Test Loss: 1.0549 | Test Acc: 62.79%\n",
            "Epoch [3/100] Train Loss: 0.9531 | Test Loss: 0.9571 | Test Acc: 65.28%\n",
            "Epoch [4/100] Train Loss: 0.7966 | Test Loss: 0.8150 | Test Acc: 72.26%\n",
            "Epoch [5/100] Train Loss: 0.6881 | Test Loss: 0.7928 | Test Acc: 73.06%\n",
            "Epoch [6/100] Train Loss: 0.5889 | Test Loss: 0.7334 | Test Acc: 74.21%\n",
            "Epoch [7/100] Train Loss: 0.4986 | Test Loss: 0.7466 | Test Acc: 75.08%\n",
            "Epoch [8/100] Train Loss: 0.4090 | Test Loss: 0.8115 | Test Acc: 74.80%\n",
            "Epoch [9/100] Train Loss: 0.3418 | Test Loss: 0.7876 | Test Acc: 75.74%\n",
            "Epoch [10/100] Train Loss: 0.2686 | Test Loss: 0.8651 | Test Acc: 75.58%\n",
            "Epoch [11/100] Train Loss: 0.2039 | Test Loss: 0.9233 | Test Acc: 75.53%\n",
            "Epoch [12/100] Train Loss: 0.1629 | Test Loss: 0.9304 | Test Acc: 75.98%\n",
            "Epoch [13/100] Train Loss: 0.1293 | Test Loss: 1.0733 | Test Acc: 75.42%\n",
            "Epoch [14/100] Train Loss: 0.1007 | Test Loss: 1.1130 | Test Acc: 75.86%\n",
            "Epoch [15/100] Train Loss: 0.0820 | Test Loss: 1.2126 | Test Acc: 75.38%\n",
            "Epoch [16/100] Train Loss: 0.0765 | Test Loss: 1.3011 | Test Acc: 76.06%\n",
            "Epoch [17/100] Train Loss: 0.0627 | Test Loss: 1.2525 | Test Acc: 76.06%\n",
            "Epoch [18/100] Train Loss: 0.0627 | Test Loss: 1.3266 | Test Acc: 75.43%\n",
            "Epoch [19/100] Train Loss: 0.0532 | Test Loss: 1.4790 | Test Acc: 75.66%\n",
            "Epoch [20/100] Train Loss: 0.0444 | Test Loss: 1.5149 | Test Acc: 75.96%\n",
            "Epoch [21/100] Train Loss: 0.0370 | Test Loss: 1.5561 | Test Acc: 75.52%\n",
            "Epoch [22/100] Train Loss: 0.0364 | Test Loss: 1.5452 | Test Acc: 76.12%\n",
            "Epoch [23/100] Train Loss: 0.0494 | Test Loss: 1.6403 | Test Acc: 75.30%\n",
            "Epoch [24/100] Train Loss: 0.0412 | Test Loss: 1.5848 | Test Acc: 75.73%\n",
            "Epoch [25/100] Train Loss: 0.0318 | Test Loss: 1.7081 | Test Acc: 75.34%\n",
            "Epoch [26/100] Train Loss: 0.0405 | Test Loss: 1.7724 | Test Acc: 75.04%\n",
            "Epoch [27/100] Train Loss: 0.0393 | Test Loss: 1.7699 | Test Acc: 75.72%\n",
            "Epoch [28/100] Train Loss: 0.0291 | Test Loss: 1.8713 | Test Acc: 75.59%\n",
            "Epoch [29/100] Train Loss: 0.0337 | Test Loss: 1.7976 | Test Acc: 75.89%\n",
            "Epoch [30/100] Train Loss: 0.0342 | Test Loss: 1.7829 | Test Acc: 75.34%\n",
            "Epoch [31/100] Train Loss: 0.0282 | Test Loss: 1.8589 | Test Acc: 75.08%\n",
            "Epoch [32/100] Train Loss: 0.0302 | Test Loss: 1.8630 | Test Acc: 75.54%\n",
            "Epoch [33/100] Train Loss: 0.0262 | Test Loss: 1.8151 | Test Acc: 75.73%\n",
            "Epoch [34/100] Train Loss: 0.0219 | Test Loss: 1.8978 | Test Acc: 76.44%\n",
            "Epoch [35/100] Train Loss: 0.0260 | Test Loss: 1.8680 | Test Acc: 76.17%\n",
            "Epoch [36/100] Train Loss: 0.0215 | Test Loss: 2.0269 | Test Acc: 76.13%\n",
            "Epoch [37/100] Train Loss: 0.0233 | Test Loss: 1.9571 | Test Acc: 76.11%\n",
            "Epoch [38/100] Train Loss: 0.0185 | Test Loss: 1.9951 | Test Acc: 76.12%\n",
            "Epoch [39/100] Train Loss: 0.0116 | Test Loss: 2.0630 | Test Acc: 76.60%\n",
            "Epoch [40/100] Train Loss: 0.0184 | Test Loss: 1.9849 | Test Acc: 75.60%\n",
            "Epoch [41/100] Train Loss: 0.0198 | Test Loss: 2.0427 | Test Acc: 75.84%\n",
            "Epoch [42/100] Train Loss: 0.0132 | Test Loss: 2.0541 | Test Acc: 76.28%\n",
            "Epoch [43/100] Train Loss: 0.0085 | Test Loss: 2.0194 | Test Acc: 76.68%\n",
            "Epoch [44/100] Train Loss: 0.0014 | Test Loss: 2.0553 | Test Acc: 77.61%\n",
            "Epoch [45/100] Train Loss: 0.0002 | Test Loss: 2.0540 | Test Acc: 77.63%\n",
            "Epoch [46/100] Train Loss: 0.0001 | Test Loss: 2.0578 | Test Acc: 77.87%\n",
            "Epoch [47/100] Train Loss: 0.0001 | Test Loss: 2.0717 | Test Acc: 77.97%\n",
            "Epoch [48/100] Train Loss: 0.0001 | Test Loss: 2.0837 | Test Acc: 77.98%\n",
            "Epoch [49/100] Train Loss: 0.0000 | Test Loss: 2.0947 | Test Acc: 77.95%\n",
            "Epoch [50/100] Train Loss: 0.0000 | Test Loss: 2.1042 | Test Acc: 77.98%\n",
            "Epoch [51/100] Train Loss: 0.0000 | Test Loss: 2.1128 | Test Acc: 77.99%\n",
            "Epoch [52/100] Train Loss: 0.0000 | Test Loss: 2.1206 | Test Acc: 77.99%\n",
            "Epoch [53/100] Train Loss: 0.0000 | Test Loss: 2.1286 | Test Acc: 77.99%\n",
            "Epoch [54/100] Train Loss: 0.0000 | Test Loss: 2.1355 | Test Acc: 77.98%\n",
            "Epoch [55/100] Train Loss: 0.0000 | Test Loss: 2.1422 | Test Acc: 78.00%\n",
            "Epoch [56/100] Train Loss: 0.0000 | Test Loss: 2.1483 | Test Acc: 78.01%\n",
            "Epoch [57/100] Train Loss: 0.0000 | Test Loss: 2.1540 | Test Acc: 78.00%\n",
            "Epoch [58/100] Train Loss: 0.0000 | Test Loss: 2.1593 | Test Acc: 78.03%\n",
            "Epoch [59/100] Train Loss: 0.0000 | Test Loss: 2.1651 | Test Acc: 78.01%\n",
            "Epoch [60/100] Train Loss: 0.0000 | Test Loss: 2.1703 | Test Acc: 78.05%\n",
            "Epoch [61/100] Train Loss: 0.0000 | Test Loss: 2.1750 | Test Acc: 78.01%\n",
            "Epoch [62/100] Train Loss: 0.0000 | Test Loss: 2.1795 | Test Acc: 78.01%\n",
            "Epoch [63/100] Train Loss: 0.0000 | Test Loss: 2.1840 | Test Acc: 77.99%\n",
            "Epoch [64/100] Train Loss: 0.0000 | Test Loss: 2.1884 | Test Acc: 77.99%\n",
            "Epoch [65/100] Train Loss: 0.0000 | Test Loss: 2.1926 | Test Acc: 78.04%\n",
            "Epoch [66/100] Train Loss: 0.0000 | Test Loss: 2.1966 | Test Acc: 78.04%\n",
            "Epoch [67/100] Train Loss: 0.0000 | Test Loss: 2.2005 | Test Acc: 78.04%\n",
            "Epoch [68/100] Train Loss: 0.0000 | Test Loss: 2.2043 | Test Acc: 78.08%\n",
            "Epoch [69/100] Train Loss: 0.0000 | Test Loss: 2.2081 | Test Acc: 78.08%\n",
            "Epoch [70/100] Train Loss: 0.0000 | Test Loss: 2.2115 | Test Acc: 78.09%\n",
            "Epoch [71/100] Train Loss: 0.0000 | Test Loss: 2.2151 | Test Acc: 78.10%\n",
            "Epoch [72/100] Train Loss: 0.0000 | Test Loss: 2.2184 | Test Acc: 78.13%\n",
            "Epoch [73/100] Train Loss: 0.0000 | Test Loss: 2.2219 | Test Acc: 78.11%\n",
            "Epoch [74/100] Train Loss: 0.0000 | Test Loss: 2.2249 | Test Acc: 78.09%\n",
            "Epoch [75/100] Train Loss: 0.0000 | Test Loss: 2.2281 | Test Acc: 78.11%\n",
            "Epoch [76/100] Train Loss: 0.0000 | Test Loss: 2.2312 | Test Acc: 78.09%\n",
            "Epoch [77/100] Train Loss: 0.0000 | Test Loss: 2.2341 | Test Acc: 78.10%\n",
            "Epoch [78/100] Train Loss: 0.0000 | Test Loss: 2.2369 | Test Acc: 78.11%\n",
            "Epoch [79/100] Train Loss: 0.0000 | Test Loss: 2.2398 | Test Acc: 78.11%\n",
            "Epoch [80/100] Train Loss: 0.0000 | Test Loss: 2.2426 | Test Acc: 78.12%\n",
            "Epoch [81/100] Train Loss: 0.0000 | Test Loss: 2.2453 | Test Acc: 78.12%\n",
            "Epoch [82/100] Train Loss: 0.0000 | Test Loss: 2.2480 | Test Acc: 78.13%\n",
            "Epoch [83/100] Train Loss: 0.0000 | Test Loss: 2.2506 | Test Acc: 78.13%\n",
            "Epoch [84/100] Train Loss: 0.0000 | Test Loss: 2.2532 | Test Acc: 78.12%\n",
            "Epoch [85/100] Train Loss: 0.0000 | Test Loss: 2.2558 | Test Acc: 78.12%\n",
            "Epoch [86/100] Train Loss: 0.0000 | Test Loss: 2.2581 | Test Acc: 78.13%\n",
            "Epoch [87/100] Train Loss: 0.0000 | Test Loss: 2.2607 | Test Acc: 78.12%\n",
            "Epoch [88/100] Train Loss: 0.0000 | Test Loss: 2.2632 | Test Acc: 78.17%\n",
            "Epoch [89/100] Train Loss: 0.0000 | Test Loss: 2.2654 | Test Acc: 78.18%\n",
            "Epoch [90/100] Train Loss: 0.0000 | Test Loss: 2.2679 | Test Acc: 78.17%\n",
            "Epoch [91/100] Train Loss: 0.0000 | Test Loss: 2.2703 | Test Acc: 78.19%\n",
            "Epoch [92/100] Train Loss: 0.0000 | Test Loss: 2.2725 | Test Acc: 78.17%\n",
            "Epoch [93/100] Train Loss: 0.0000 | Test Loss: 2.2747 | Test Acc: 78.19%\n",
            "Epoch [94/100] Train Loss: 0.0000 | Test Loss: 2.2768 | Test Acc: 78.20%\n",
            "Epoch [95/100] Train Loss: 0.0000 | Test Loss: 2.2790 | Test Acc: 78.20%\n",
            "Epoch [96/100] Train Loss: 0.0000 | Test Loss: 2.2810 | Test Acc: 78.20%\n",
            "Epoch [97/100] Train Loss: 0.0000 | Test Loss: 2.2831 | Test Acc: 78.20%\n",
            "Epoch [98/100] Train Loss: 0.0000 | Test Loss: 2.2850 | Test Acc: 78.23%\n",
            "Epoch [99/100] Train Loss: 0.0000 | Test Loss: 2.2870 | Test Acc: 78.23%\n",
            "Epoch [100/100] Train Loss: 0.0000 | Test Loss: 2.2891 | Test Acc: 78.26%\n",
            "\n",
            "=== Final Results for Problem 1.b (Deeper CNN, 100 epochs) ===\n",
            "Training Time: 761.29 seconds\n",
            "Final Training Loss: 0.0000\n",
            "Final Test Loss: 2.2891\n",
            "Final Test Accuracy: 78.26%\n",
            "Total Parameters: 620362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKOtScGqeuc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}