{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joanoT5JpFH2",
        "outputId": "4eee68ed-b521-4445-c57b-975fb09149e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:18<00:00, 9.16MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 50000\n",
            "Test samples: 10000\n"
          ]
        }
      ],
      "source": [
        "# Mohammadmilad Sayyad\n",
        "# Problem 2.b\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import time\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size    = 128\n",
        "learning_rate = 0.01\n",
        "num_epochs    = 100   # <<< 100 epochs for Problem 2.b\n",
        "\n",
        "# CIFAR-10 normalization\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std  = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# CIFAR-10 datasets and loaders\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels,\n",
        "                                      kernel_size=1, stride=stride, bias=False)\n",
        "        else:\n",
        "            self.shortcut = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        identity = x if self.shortcut is None else self.shortcut(x)\n",
        "        out = F.relu(out + identity)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "\n",
        "        self.in_channels = 16\n",
        "\n",
        "        # Initial conv\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # 10 blocks total: 3 + 3 + 4\n",
        "        self.layer1 = self._make_layer(16, num_blocks=3, stride=1)  # 32x32\n",
        "        self.layer2 = self._make_layer(32, num_blocks=3, stride=2)  # 16x16\n",
        "        self.layer3 = self._make_layer(64, num_blocks=4, stride=2)  # 8x8\n",
        "\n",
        "        # Global average pooling + FC\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)  # (N, 64)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "NEokMaNCpdUh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlockBN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlockBN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        identity = x if self.shortcut is None else self.shortcut(x)\n",
        "        out = F.relu(out + identity)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet10_BN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet10_BN, self).__init__()\n",
        "\n",
        "        self.in_channels = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.layer1 = self._make_layer(16, num_blocks=3, stride=1)\n",
        "        self.layer2 = self._make_layer(32, num_blocks=3, stride=2)\n",
        "        self.layer3 = self._make_layer(64, num_blocks=4, stride=2)\n",
        "\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(ResidualBlockBN(self.in_channels, out_channels, stride=s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "0SaTsuaLpeaR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet10_Dropout(nn.Module):\n",
        "    def __init__(self, num_classes=10, p=0.3):\n",
        "        super(ResNet10_Dropout, self).__init__()\n",
        "\n",
        "        self.in_channels = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.layer1 = self._make_layer(16, num_blocks=3, stride=1)\n",
        "        self.layer2 = self._make_layer(32, num_blocks=3, stride=2)\n",
        "        self.layer3 = self._make_layer(64, num_blocks=4, stride=2)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=p)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "IQPzMO0epgEJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, criterion, dataloader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader.dataset)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def run_experiment(model, optimizer, num_epochs, label):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_params = count_parameters(model)\n",
        "    print(f\"\\n===== {label} =====\")\n",
        "    print(f\"Model parameters: {num_params}\")\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(f\"Training for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
        "        test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        print(f\"[{label}] Epoch [{epoch+1}/{num_epochs}] \"\n",
        "              f\"Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Test Loss: {test_loss:.4f} | \"\n",
        "              f\"Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    print(f\"\\n=== Final Results: {label} ===\")\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "    print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"Final test loss: {test_losses[-1]:.4f}\")\n",
        "    print(f\"Final test accuracy: {test_accuracies[-1]:.2f}%\")\n",
        "    print(f\"Model size (parameters): {num_params}\")\n",
        "\n",
        "    return {\n",
        "        \"label\": label,\n",
        "        \"time\": training_time,\n",
        "        \"train_loss\": train_losses[-1],\n",
        "        \"test_loss\": test_losses[-1],\n",
        "        \"test_acc\": test_accuracies[-1],\n",
        "        \"params\": num_params,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "wMBfy60_phkx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "# 1) Weight Decay λ = 0.001 (no BN, no dropout)\n",
        "model_wd = ResNet10(num_classes=10)\n",
        "optimizer_wd = optim.SGD(model_wd.parameters(),\n",
        "                         lr=learning_rate,\n",
        "                         momentum=0.9,\n",
        "                         weight_decay=0.001)   # <<< L2 / weight decay\n",
        "\n",
        "res_wd = run_experiment(model_wd, optimizer_wd, num_epochs, label=\"ResNet-10 + Weight Decay (λ=0.001)\")\n",
        "results.append(res_wd)\n",
        "\n",
        "\n",
        "# 2) Dropout p = 0.3 (no BN, no weight_decay)\n",
        "model_do = ResNet10_Dropout(num_classes=10, p=0.3)\n",
        "optimizer_do = optim.SGD(model_do.parameters(),\n",
        "                         lr=learning_rate,\n",
        "                         momentum=0.9,\n",
        "                         weight_decay=0.0)\n",
        "\n",
        "res_do = run_experiment(model_do, optimizer_do, num_epochs, label=\"ResNet-10 + Dropout (p=0.3)\")\n",
        "results.append(res_do)\n",
        "\n",
        "\n",
        "# 3) Batch Normalization (no dropout, no weight_decay)\n",
        "model_bn = ResNet10_BN(num_classes=10)\n",
        "optimizer_bn = optim.SGD(model_bn.parameters(),\n",
        "                         lr=learning_rate,\n",
        "                         momentum=0.9,\n",
        "                         weight_decay=0.0)\n",
        "\n",
        "res_bn = run_experiment(model_bn, optimizer_bn, num_epochs, label=\"ResNet-10 + BatchNorm\")\n",
        "results.append(res_bn)\n",
        "\n",
        "print(\"\\n===== Summary of All 3 Regularization Experiments =====\")\n",
        "for r in results:\n",
        "    print(f\"{r['label']}: \"\n",
        "          f\"time={r['time']:.2f}s, \"\n",
        "          f\"train_loss={r['train_loss']:.4f}, \"\n",
        "          f\"test_acc={r['test_acc']:.2f}%, \"\n",
        "          f\"params={r['params']}\")\n"
      ],
      "metadata": {
        "id": "3BQht5QZpit4",
        "outputId": "3c518e66-cf3c-4490-a6e4-e115f0c0dbe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ResNet-10 + Weight Decay (λ=0.001) =====\n",
            "Model parameters: 344634\n",
            "Training for 100 epochs...\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [1/100] Train Loss: 2.0554 | Test Loss: 1.8224 | Test Acc: 32.14%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [2/100] Train Loss: 1.7126 | Test Loss: 1.5542 | Test Acc: 43.54%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [3/100] Train Loss: 1.5323 | Test Loss: 1.4446 | Test Acc: 46.06%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [4/100] Train Loss: 1.4092 | Test Loss: 1.4337 | Test Acc: 48.31%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [5/100] Train Loss: 1.2690 | Test Loss: 1.1980 | Test Acc: 55.69%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [6/100] Train Loss: 1.1460 | Test Loss: 1.1526 | Test Acc: 58.68%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [7/100] Train Loss: 1.0618 | Test Loss: 1.0664 | Test Acc: 62.73%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [8/100] Train Loss: 0.9869 | Test Loss: 0.9777 | Test Acc: 65.87%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [9/100] Train Loss: 0.9129 | Test Loss: 0.9044 | Test Acc: 68.24%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [10/100] Train Loss: 0.8648 | Test Loss: 0.9051 | Test Acc: 68.17%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [11/100] Train Loss: 0.8047 | Test Loss: 0.7942 | Test Acc: 72.56%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [12/100] Train Loss: 0.7519 | Test Loss: 0.8590 | Test Acc: 69.76%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [13/100] Train Loss: 0.7142 | Test Loss: 0.7455 | Test Acc: 74.20%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [14/100] Train Loss: 0.6717 | Test Loss: 0.7555 | Test Acc: 74.51%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [15/100] Train Loss: 0.6375 | Test Loss: 0.7778 | Test Acc: 73.43%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [16/100] Train Loss: 0.6088 | Test Loss: 0.7026 | Test Acc: 76.01%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [17/100] Train Loss: 0.5704 | Test Loss: 0.6384 | Test Acc: 78.33%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [18/100] Train Loss: 0.5504 | Test Loss: 0.6718 | Test Acc: 77.10%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [19/100] Train Loss: 0.5160 | Test Loss: 0.6735 | Test Acc: 77.45%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [20/100] Train Loss: 0.4961 | Test Loss: 0.6325 | Test Acc: 79.46%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [21/100] Train Loss: 0.4810 | Test Loss: 0.6510 | Test Acc: 78.47%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [22/100] Train Loss: 0.4356 | Test Loss: 0.6035 | Test Acc: 80.06%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [23/100] Train Loss: 0.4271 | Test Loss: 0.6612 | Test Acc: 79.09%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [24/100] Train Loss: 0.4050 | Test Loss: 0.6810 | Test Acc: 78.66%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [25/100] Train Loss: 0.4058 | Test Loss: 0.5858 | Test Acc: 80.97%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [26/100] Train Loss: 0.3734 | Test Loss: 0.6110 | Test Acc: 80.83%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [27/100] Train Loss: 0.3526 | Test Loss: 0.6233 | Test Acc: 80.76%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [28/100] Train Loss: 0.3395 | Test Loss: 0.6182 | Test Acc: 81.73%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [29/100] Train Loss: 0.3222 | Test Loss: 0.7266 | Test Acc: 79.76%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [30/100] Train Loss: 0.3179 | Test Loss: 0.5893 | Test Acc: 81.11%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [31/100] Train Loss: 0.3027 | Test Loss: 0.5814 | Test Acc: 81.68%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [32/100] Train Loss: 0.2859 | Test Loss: 0.6368 | Test Acc: 79.42%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [33/100] Train Loss: 0.2720 | Test Loss: 0.6189 | Test Acc: 81.08%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [34/100] Train Loss: 0.2605 | Test Loss: 0.6164 | Test Acc: 81.14%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [35/100] Train Loss: 0.2443 | Test Loss: 0.6113 | Test Acc: 80.93%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [36/100] Train Loss: 0.2405 | Test Loss: 0.6717 | Test Acc: 79.54%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [37/100] Train Loss: 0.2327 | Test Loss: 0.6537 | Test Acc: 80.27%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [38/100] Train Loss: 0.2235 | Test Loss: 0.6586 | Test Acc: 80.69%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [39/100] Train Loss: 0.2215 | Test Loss: 0.7241 | Test Acc: 80.88%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [40/100] Train Loss: 0.2094 | Test Loss: 0.6958 | Test Acc: 80.40%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [41/100] Train Loss: 0.2069 | Test Loss: 0.7196 | Test Acc: 80.34%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [42/100] Train Loss: 0.1889 | Test Loss: 0.7015 | Test Acc: 80.91%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [43/100] Train Loss: 0.1799 | Test Loss: 0.7737 | Test Acc: 78.77%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [44/100] Train Loss: 0.1779 | Test Loss: 0.6444 | Test Acc: 81.62%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [45/100] Train Loss: 0.1741 | Test Loss: 0.7607 | Test Acc: 80.26%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [46/100] Train Loss: 0.1616 | Test Loss: 0.8358 | Test Acc: 79.43%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [47/100] Train Loss: 0.1657 | Test Loss: 0.6562 | Test Acc: 82.75%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [48/100] Train Loss: 0.1523 | Test Loss: 0.7167 | Test Acc: 81.27%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [49/100] Train Loss: 0.1442 | Test Loss: 0.8156 | Test Acc: 79.07%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [50/100] Train Loss: 0.1522 | Test Loss: 0.7384 | Test Acc: 80.74%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [51/100] Train Loss: 0.1556 | Test Loss: 0.7169 | Test Acc: 81.32%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [52/100] Train Loss: 0.1311 | Test Loss: 0.7986 | Test Acc: 81.36%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [53/100] Train Loss: 0.1319 | Test Loss: 0.8235 | Test Acc: 81.23%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [54/100] Train Loss: 0.1346 | Test Loss: 0.7438 | Test Acc: 81.50%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [55/100] Train Loss: 0.1368 | Test Loss: 0.7583 | Test Acc: 81.32%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [56/100] Train Loss: 0.1314 | Test Loss: 0.8487 | Test Acc: 80.20%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [57/100] Train Loss: 0.1346 | Test Loss: 0.8194 | Test Acc: 80.12%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [58/100] Train Loss: 0.1134 | Test Loss: 0.8919 | Test Acc: 78.72%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [59/100] Train Loss: 0.1348 | Test Loss: 0.7690 | Test Acc: 81.11%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [60/100] Train Loss: 0.1270 | Test Loss: 0.7533 | Test Acc: 80.42%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [61/100] Train Loss: 0.1315 | Test Loss: 0.7346 | Test Acc: 81.47%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [62/100] Train Loss: 0.1094 | Test Loss: 0.7356 | Test Acc: 80.92%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [63/100] Train Loss: 0.1166 | Test Loss: 0.8110 | Test Acc: 80.66%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [64/100] Train Loss: 0.1143 | Test Loss: 0.8312 | Test Acc: 80.35%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [65/100] Train Loss: 0.1062 | Test Loss: 0.8636 | Test Acc: 79.99%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [66/100] Train Loss: 0.1155 | Test Loss: 0.9019 | Test Acc: 80.09%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [67/100] Train Loss: 0.1056 | Test Loss: 0.7478 | Test Acc: 81.74%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [68/100] Train Loss: 0.1001 | Test Loss: 0.8778 | Test Acc: 80.59%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [69/100] Train Loss: 0.1191 | Test Loss: 0.8504 | Test Acc: 80.26%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [70/100] Train Loss: 0.1080 | Test Loss: 0.7234 | Test Acc: 81.81%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [71/100] Train Loss: 0.1122 | Test Loss: 0.7466 | Test Acc: 80.73%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [72/100] Train Loss: 0.1086 | Test Loss: 0.7692 | Test Acc: 81.05%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [73/100] Train Loss: 0.1014 | Test Loss: 0.9288 | Test Acc: 81.89%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [74/100] Train Loss: 0.1128 | Test Loss: 0.7627 | Test Acc: 81.24%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [75/100] Train Loss: 0.1036 | Test Loss: 1.0381 | Test Acc: 76.29%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [76/100] Train Loss: 0.1060 | Test Loss: 0.8040 | Test Acc: 81.07%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [77/100] Train Loss: 0.1021 | Test Loss: 0.8010 | Test Acc: 81.87%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [78/100] Train Loss: 0.1059 | Test Loss: 0.8182 | Test Acc: 81.67%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [79/100] Train Loss: 0.0983 | Test Loss: 0.8488 | Test Acc: 81.26%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [80/100] Train Loss: 0.0975 | Test Loss: 0.8312 | Test Acc: 81.61%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [81/100] Train Loss: 0.0995 | Test Loss: 0.8764 | Test Acc: 80.15%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [82/100] Train Loss: 0.1015 | Test Loss: 0.8943 | Test Acc: 80.47%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [83/100] Train Loss: 0.1039 | Test Loss: 0.8657 | Test Acc: 79.61%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [84/100] Train Loss: 0.0990 | Test Loss: 0.8992 | Test Acc: 79.94%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [85/100] Train Loss: 0.0867 | Test Loss: 0.8055 | Test Acc: 82.28%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [86/100] Train Loss: 0.0957 | Test Loss: 0.7242 | Test Acc: 80.91%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [87/100] Train Loss: 0.1022 | Test Loss: 0.8339 | Test Acc: 79.85%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [88/100] Train Loss: 0.0972 | Test Loss: 0.8759 | Test Acc: 81.62%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [89/100] Train Loss: 0.0892 | Test Loss: 0.7971 | Test Acc: 80.83%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [90/100] Train Loss: 0.0986 | Test Loss: 0.8015 | Test Acc: 81.13%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [91/100] Train Loss: 0.0996 | Test Loss: 0.7859 | Test Acc: 82.42%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [92/100] Train Loss: 0.0881 | Test Loss: 0.8459 | Test Acc: 81.44%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [93/100] Train Loss: 0.0872 | Test Loss: 0.8571 | Test Acc: 81.36%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [94/100] Train Loss: 0.1015 | Test Loss: 0.8250 | Test Acc: 80.62%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [95/100] Train Loss: 0.0949 | Test Loss: 0.7684 | Test Acc: 82.14%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [96/100] Train Loss: 0.0912 | Test Loss: 0.8974 | Test Acc: 80.18%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [97/100] Train Loss: 0.0988 | Test Loss: 0.7568 | Test Acc: 81.74%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [98/100] Train Loss: 0.0892 | Test Loss: 0.7946 | Test Acc: 81.84%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [99/100] Train Loss: 0.0881 | Test Loss: 0.8489 | Test Acc: 80.53%\n",
            "[ResNet-10 + Weight Decay (λ=0.001)] Epoch [100/100] Train Loss: 0.0929 | Test Loss: 0.7649 | Test Acc: 81.49%\n",
            "\n",
            "=== Final Results: ResNet-10 + Weight Decay (λ=0.001) ===\n",
            "Training time: 767.54 seconds\n",
            "Final training loss: 0.0929\n",
            "Final test loss: 0.7649\n",
            "Final test accuracy: 81.49%\n",
            "Model size (parameters): 344634\n",
            "\n",
            "===== ResNet-10 + Dropout (p=0.3) =====\n",
            "Model parameters: 344634\n",
            "Training for 100 epochs...\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [1/100] Train Loss: 2.0935 | Test Loss: 1.8568 | Test Acc: 29.74%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [2/100] Train Loss: 1.7811 | Test Loss: 1.7020 | Test Acc: 33.70%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [3/100] Train Loss: 1.6223 | Test Loss: 1.5134 | Test Acc: 43.29%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [4/100] Train Loss: 1.5069 | Test Loss: 1.4382 | Test Acc: 47.60%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [5/100] Train Loss: 1.3741 | Test Loss: 1.3177 | Test Acc: 53.38%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [6/100] Train Loss: 1.2630 | Test Loss: 1.2639 | Test Acc: 53.16%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [7/100] Train Loss: 1.1629 | Test Loss: 1.1258 | Test Acc: 60.08%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [8/100] Train Loss: 1.0863 | Test Loss: 1.0733 | Test Acc: 62.37%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [9/100] Train Loss: 1.0177 | Test Loss: 0.9387 | Test Acc: 67.28%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [10/100] Train Loss: 0.9531 | Test Loss: 0.9032 | Test Acc: 68.51%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [11/100] Train Loss: 0.8933 | Test Loss: 0.8522 | Test Acc: 70.33%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [12/100] Train Loss: 0.8403 | Test Loss: 0.8840 | Test Acc: 69.24%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [13/100] Train Loss: 0.7849 | Test Loss: 0.8233 | Test Acc: 71.90%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [14/100] Train Loss: 0.7372 | Test Loss: 0.7693 | Test Acc: 73.32%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [15/100] Train Loss: 0.6938 | Test Loss: 0.7836 | Test Acc: 73.74%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [16/100] Train Loss: 0.6587 | Test Loss: 0.7173 | Test Acc: 75.62%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [17/100] Train Loss: 0.6190 | Test Loss: 0.7052 | Test Acc: 76.14%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [18/100] Train Loss: 0.5809 | Test Loss: 0.6898 | Test Acc: 76.48%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [19/100] Train Loss: 0.5523 | Test Loss: 0.7168 | Test Acc: 76.66%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [20/100] Train Loss: 0.5174 | Test Loss: 0.6963 | Test Acc: 77.52%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [21/100] Train Loss: 0.4833 | Test Loss: 0.6483 | Test Acc: 79.22%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [22/100] Train Loss: 0.4593 | Test Loss: 0.6472 | Test Acc: 79.56%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [23/100] Train Loss: 0.4333 | Test Loss: 0.6514 | Test Acc: 79.12%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [24/100] Train Loss: 0.4008 | Test Loss: 0.6957 | Test Acc: 78.50%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [25/100] Train Loss: 0.3822 | Test Loss: 0.7032 | Test Acc: 78.52%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [26/100] Train Loss: 0.3548 | Test Loss: 0.6813 | Test Acc: 79.03%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [27/100] Train Loss: 0.3514 | Test Loss: 0.7251 | Test Acc: 77.29%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [28/100] Train Loss: 0.3269 | Test Loss: 0.7166 | Test Acc: 80.04%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [29/100] Train Loss: 0.3030 | Test Loss: 0.7110 | Test Acc: 79.36%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [30/100] Train Loss: 0.2912 | Test Loss: 0.7970 | Test Acc: 78.79%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [31/100] Train Loss: 0.2696 | Test Loss: 0.7087 | Test Acc: 79.24%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [32/100] Train Loss: 0.2614 | Test Loss: 0.8554 | Test Acc: 78.40%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [33/100] Train Loss: 0.2466 | Test Loss: 0.7875 | Test Acc: 79.15%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [34/100] Train Loss: 0.2386 | Test Loss: 0.8110 | Test Acc: 79.86%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [35/100] Train Loss: 0.2260 | Test Loss: 0.9157 | Test Acc: 78.42%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [36/100] Train Loss: 0.2100 | Test Loss: 0.8836 | Test Acc: 78.50%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [37/100] Train Loss: 0.2047 | Test Loss: 0.8924 | Test Acc: 79.52%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [38/100] Train Loss: 0.2070 | Test Loss: 0.7952 | Test Acc: 78.55%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [39/100] Train Loss: 0.1907 | Test Loss: 0.8763 | Test Acc: 79.29%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [40/100] Train Loss: 0.1813 | Test Loss: 0.8805 | Test Acc: 79.47%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [41/100] Train Loss: 0.1683 | Test Loss: 0.9202 | Test Acc: 79.01%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [42/100] Train Loss: 0.1577 | Test Loss: 1.0143 | Test Acc: 79.90%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [43/100] Train Loss: 0.1612 | Test Loss: 0.9246 | Test Acc: 78.35%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [44/100] Train Loss: 0.1539 | Test Loss: 0.9162 | Test Acc: 79.50%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [45/100] Train Loss: 0.1479 | Test Loss: 0.8811 | Test Acc: 78.76%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [46/100] Train Loss: 0.1395 | Test Loss: 1.0744 | Test Acc: 78.69%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [47/100] Train Loss: 0.1439 | Test Loss: 0.9881 | Test Acc: 79.06%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [48/100] Train Loss: 0.1377 | Test Loss: 1.1157 | Test Acc: 79.30%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [49/100] Train Loss: 0.1373 | Test Loss: 1.0747 | Test Acc: 79.22%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [50/100] Train Loss: 0.1264 | Test Loss: 0.9960 | Test Acc: 78.13%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [51/100] Train Loss: 0.1458 | Test Loss: 0.9656 | Test Acc: 79.85%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [52/100] Train Loss: 0.1189 | Test Loss: 1.2036 | Test Acc: 79.33%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [53/100] Train Loss: 0.1266 | Test Loss: 1.1332 | Test Acc: 78.99%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [54/100] Train Loss: 0.1182 | Test Loss: 1.1548 | Test Acc: 79.35%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [55/100] Train Loss: 0.1104 | Test Loss: 1.1943 | Test Acc: 79.43%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [56/100] Train Loss: 0.1088 | Test Loss: 1.2231 | Test Acc: 78.40%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [57/100] Train Loss: 0.1106 | Test Loss: 1.1158 | Test Acc: 80.07%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [58/100] Train Loss: 0.0987 | Test Loss: 1.1294 | Test Acc: 79.68%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [59/100] Train Loss: 0.0957 | Test Loss: 1.1343 | Test Acc: 80.06%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [60/100] Train Loss: 0.1042 | Test Loss: 1.1975 | Test Acc: 79.09%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [61/100] Train Loss: 0.1095 | Test Loss: 1.2783 | Test Acc: 78.79%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [62/100] Train Loss: 0.1025 | Test Loss: 1.2240 | Test Acc: 77.43%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [63/100] Train Loss: 0.0905 | Test Loss: 1.2150 | Test Acc: 78.84%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [64/100] Train Loss: 0.1022 | Test Loss: 1.2537 | Test Acc: 78.75%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [65/100] Train Loss: 0.0963 | Test Loss: 1.2000 | Test Acc: 78.65%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [66/100] Train Loss: 0.0963 | Test Loss: 1.1481 | Test Acc: 79.56%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [67/100] Train Loss: 0.0853 | Test Loss: 1.3136 | Test Acc: 79.32%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [68/100] Train Loss: 0.0854 | Test Loss: 1.2898 | Test Acc: 78.87%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [69/100] Train Loss: 0.0932 | Test Loss: 1.0219 | Test Acc: 78.76%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [70/100] Train Loss: 0.0907 | Test Loss: 1.1580 | Test Acc: 79.81%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [71/100] Train Loss: 0.0734 | Test Loss: 1.1576 | Test Acc: 79.51%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [72/100] Train Loss: 0.0805 | Test Loss: 1.2475 | Test Acc: 78.15%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [73/100] Train Loss: 0.0812 | Test Loss: 1.2622 | Test Acc: 79.31%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [74/100] Train Loss: 0.0815 | Test Loss: 1.2312 | Test Acc: 79.38%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [75/100] Train Loss: 0.0742 | Test Loss: 1.2486 | Test Acc: 79.50%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [76/100] Train Loss: 0.0836 | Test Loss: 1.3261 | Test Acc: 78.31%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [77/100] Train Loss: 0.0830 | Test Loss: 1.1751 | Test Acc: 79.77%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [78/100] Train Loss: 0.0650 | Test Loss: 1.4961 | Test Acc: 79.00%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [79/100] Train Loss: 0.0782 | Test Loss: 1.3466 | Test Acc: 79.87%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [80/100] Train Loss: 0.0671 | Test Loss: 1.4599 | Test Acc: 79.60%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [81/100] Train Loss: 0.0671 | Test Loss: 1.2999 | Test Acc: 79.59%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [82/100] Train Loss: 0.0735 | Test Loss: 1.2752 | Test Acc: 78.59%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [83/100] Train Loss: 0.0683 | Test Loss: 1.3137 | Test Acc: 79.38%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [84/100] Train Loss: 0.0682 | Test Loss: 1.3754 | Test Acc: 79.38%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [85/100] Train Loss: 0.0669 | Test Loss: 1.4621 | Test Acc: 78.82%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [86/100] Train Loss: 0.0628 | Test Loss: 1.3239 | Test Acc: 78.78%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [87/100] Train Loss: 0.0718 | Test Loss: 1.1823 | Test Acc: 79.60%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [88/100] Train Loss: 0.0706 | Test Loss: 1.3823 | Test Acc: 79.21%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [89/100] Train Loss: 0.0748 | Test Loss: 1.2630 | Test Acc: 79.04%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [90/100] Train Loss: 0.0626 | Test Loss: 1.3160 | Test Acc: 79.16%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [91/100] Train Loss: 0.0598 | Test Loss: 1.3676 | Test Acc: 78.59%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [92/100] Train Loss: 0.0660 | Test Loss: 1.3221 | Test Acc: 80.18%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [93/100] Train Loss: 0.0570 | Test Loss: 1.3613 | Test Acc: 79.38%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [94/100] Train Loss: 0.0617 | Test Loss: 1.4378 | Test Acc: 78.43%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [95/100] Train Loss: 0.0671 | Test Loss: 1.3098 | Test Acc: 79.62%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [96/100] Train Loss: 0.0605 | Test Loss: 1.3616 | Test Acc: 78.42%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [97/100] Train Loss: 0.0564 | Test Loss: 1.3037 | Test Acc: 78.69%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [98/100] Train Loss: 0.0480 | Test Loss: 1.4397 | Test Acc: 78.86%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [99/100] Train Loss: 0.0495 | Test Loss: 1.5330 | Test Acc: 79.81%\n",
            "[ResNet-10 + Dropout (p=0.3)] Epoch [100/100] Train Loss: 0.0585 | Test Loss: 1.3672 | Test Acc: 79.69%\n",
            "\n",
            "=== Final Results: ResNet-10 + Dropout (p=0.3) ===\n",
            "Training time: 761.95 seconds\n",
            "Final training loss: 0.0585\n",
            "Final test loss: 1.3672\n",
            "Final test accuracy: 79.69%\n",
            "Model size (parameters): 344634\n",
            "\n",
            "===== ResNet-10 + BatchNorm =====\n",
            "Model parameters: 346458\n",
            "Training for 100 epochs...\n",
            "[ResNet-10 + BatchNorm] Epoch [1/100] Train Loss: 1.4696 | Test Loss: 1.2660 | Test Acc: 54.81%\n",
            "[ResNet-10 + BatchNorm] Epoch [2/100] Train Loss: 0.9646 | Test Loss: 1.1002 | Test Acc: 61.81%\n",
            "[ResNet-10 + BatchNorm] Epoch [3/100] Train Loss: 0.7517 | Test Loss: 0.7900 | Test Acc: 72.20%\n",
            "[ResNet-10 + BatchNorm] Epoch [4/100] Train Loss: 0.6306 | Test Loss: 0.9507 | Test Acc: 69.03%\n",
            "[ResNet-10 + BatchNorm] Epoch [5/100] Train Loss: 0.5418 | Test Loss: 0.7081 | Test Acc: 75.52%\n",
            "[ResNet-10 + BatchNorm] Epoch [6/100] Train Loss: 0.4775 | Test Loss: 0.7091 | Test Acc: 76.90%\n",
            "[ResNet-10 + BatchNorm] Epoch [7/100] Train Loss: 0.4201 | Test Loss: 0.6407 | Test Acc: 78.89%\n",
            "[ResNet-10 + BatchNorm] Epoch [8/100] Train Loss: 0.3740 | Test Loss: 0.7600 | Test Acc: 75.92%\n",
            "[ResNet-10 + BatchNorm] Epoch [9/100] Train Loss: 0.3288 | Test Loss: 0.7159 | Test Acc: 77.72%\n",
            "[ResNet-10 + BatchNorm] Epoch [10/100] Train Loss: 0.2836 | Test Loss: 0.6053 | Test Acc: 80.22%\n",
            "[ResNet-10 + BatchNorm] Epoch [11/100] Train Loss: 0.2507 | Test Loss: 0.7318 | Test Acc: 77.39%\n",
            "[ResNet-10 + BatchNorm] Epoch [12/100] Train Loss: 0.2237 | Test Loss: 0.6994 | Test Acc: 78.95%\n",
            "[ResNet-10 + BatchNorm] Epoch [13/100] Train Loss: 0.1904 | Test Loss: 0.8394 | Test Acc: 76.71%\n",
            "[ResNet-10 + BatchNorm] Epoch [14/100] Train Loss: 0.1651 | Test Loss: 1.1258 | Test Acc: 72.58%\n",
            "[ResNet-10 + BatchNorm] Epoch [15/100] Train Loss: 0.1409 | Test Loss: 0.8496 | Test Acc: 77.43%\n",
            "[ResNet-10 + BatchNorm] Epoch [16/100] Train Loss: 0.1271 | Test Loss: 0.8176 | Test Acc: 79.23%\n",
            "[ResNet-10 + BatchNorm] Epoch [17/100] Train Loss: 0.1121 | Test Loss: 0.9150 | Test Acc: 77.32%\n",
            "[ResNet-10 + BatchNorm] Epoch [18/100] Train Loss: 0.0983 | Test Loss: 0.8238 | Test Acc: 79.57%\n",
            "[ResNet-10 + BatchNorm] Epoch [19/100] Train Loss: 0.0881 | Test Loss: 0.9786 | Test Acc: 77.89%\n",
            "[ResNet-10 + BatchNorm] Epoch [20/100] Train Loss: 0.0758 | Test Loss: 0.8822 | Test Acc: 79.69%\n",
            "[ResNet-10 + BatchNorm] Epoch [21/100] Train Loss: 0.0731 | Test Loss: 1.0072 | Test Acc: 78.18%\n",
            "[ResNet-10 + BatchNorm] Epoch [22/100] Train Loss: 0.0478 | Test Loss: 1.0588 | Test Acc: 77.76%\n",
            "[ResNet-10 + BatchNorm] Epoch [23/100] Train Loss: 0.0474 | Test Loss: 0.9716 | Test Acc: 79.69%\n",
            "[ResNet-10 + BatchNorm] Epoch [24/100] Train Loss: 0.0504 | Test Loss: 0.9379 | Test Acc: 80.48%\n",
            "[ResNet-10 + BatchNorm] Epoch [25/100] Train Loss: 0.0377 | Test Loss: 0.9820 | Test Acc: 80.45%\n",
            "[ResNet-10 + BatchNorm] Epoch [26/100] Train Loss: 0.0316 | Test Loss: 1.1257 | Test Acc: 78.53%\n",
            "[ResNet-10 + BatchNorm] Epoch [27/100] Train Loss: 0.0346 | Test Loss: 0.9822 | Test Acc: 80.76%\n",
            "[ResNet-10 + BatchNorm] Epoch [28/100] Train Loss: 0.0390 | Test Loss: 1.0410 | Test Acc: 79.26%\n",
            "[ResNet-10 + BatchNorm] Epoch [29/100] Train Loss: 0.0357 | Test Loss: 0.9935 | Test Acc: 80.58%\n",
            "[ResNet-10 + BatchNorm] Epoch [30/100] Train Loss: 0.0265 | Test Loss: 1.0041 | Test Acc: 81.12%\n",
            "[ResNet-10 + BatchNorm] Epoch [31/100] Train Loss: 0.0299 | Test Loss: 1.0280 | Test Acc: 80.54%\n",
            "[ResNet-10 + BatchNorm] Epoch [32/100] Train Loss: 0.0262 | Test Loss: 1.0776 | Test Acc: 80.73%\n",
            "[ResNet-10 + BatchNorm] Epoch [33/100] Train Loss: 0.0204 | Test Loss: 1.0050 | Test Acc: 81.30%\n",
            "[ResNet-10 + BatchNorm] Epoch [34/100] Train Loss: 0.0170 | Test Loss: 1.0960 | Test Acc: 80.42%\n",
            "[ResNet-10 + BatchNorm] Epoch [35/100] Train Loss: 0.0119 | Test Loss: 1.0250 | Test Acc: 81.00%\n",
            "[ResNet-10 + BatchNorm] Epoch [36/100] Train Loss: 0.0123 | Test Loss: 1.0657 | Test Acc: 81.03%\n",
            "[ResNet-10 + BatchNorm] Epoch [37/100] Train Loss: 0.0097 | Test Loss: 1.0593 | Test Acc: 80.78%\n",
            "[ResNet-10 + BatchNorm] Epoch [38/100] Train Loss: 0.0094 | Test Loss: 1.0708 | Test Acc: 81.35%\n",
            "[ResNet-10 + BatchNorm] Epoch [39/100] Train Loss: 0.0066 | Test Loss: 1.0447 | Test Acc: 81.32%\n",
            "[ResNet-10 + BatchNorm] Epoch [40/100] Train Loss: 0.0047 | Test Loss: 1.0278 | Test Acc: 81.78%\n",
            "[ResNet-10 + BatchNorm] Epoch [41/100] Train Loss: 0.0037 | Test Loss: 1.0838 | Test Acc: 81.31%\n",
            "[ResNet-10 + BatchNorm] Epoch [42/100] Train Loss: 0.0017 | Test Loss: 1.0234 | Test Acc: 82.30%\n",
            "[ResNet-10 + BatchNorm] Epoch [43/100] Train Loss: 0.0008 | Test Loss: 1.0306 | Test Acc: 82.12%\n",
            "[ResNet-10 + BatchNorm] Epoch [44/100] Train Loss: 0.0006 | Test Loss: 1.0255 | Test Acc: 82.29%\n",
            "[ResNet-10 + BatchNorm] Epoch [45/100] Train Loss: 0.0005 | Test Loss: 1.0244 | Test Acc: 82.43%\n",
            "[ResNet-10 + BatchNorm] Epoch [46/100] Train Loss: 0.0004 | Test Loss: 1.0300 | Test Acc: 82.34%\n",
            "[ResNet-10 + BatchNorm] Epoch [47/100] Train Loss: 0.0003 | Test Loss: 1.0382 | Test Acc: 82.36%\n",
            "[ResNet-10 + BatchNorm] Epoch [48/100] Train Loss: 0.0003 | Test Loss: 1.0337 | Test Acc: 82.43%\n",
            "[ResNet-10 + BatchNorm] Epoch [49/100] Train Loss: 0.0003 | Test Loss: 1.0363 | Test Acc: 82.29%\n",
            "[ResNet-10 + BatchNorm] Epoch [50/100] Train Loss: 0.0003 | Test Loss: 1.0373 | Test Acc: 82.37%\n",
            "[ResNet-10 + BatchNorm] Epoch [51/100] Train Loss: 0.0003 | Test Loss: 1.0375 | Test Acc: 82.46%\n",
            "[ResNet-10 + BatchNorm] Epoch [52/100] Train Loss: 0.0003 | Test Loss: 1.0482 | Test Acc: 82.30%\n",
            "[ResNet-10 + BatchNorm] Epoch [53/100] Train Loss: 0.0003 | Test Loss: 1.0462 | Test Acc: 82.52%\n",
            "[ResNet-10 + BatchNorm] Epoch [54/100] Train Loss: 0.0002 | Test Loss: 1.0498 | Test Acc: 82.40%\n",
            "[ResNet-10 + BatchNorm] Epoch [55/100] Train Loss: 0.0002 | Test Loss: 1.0499 | Test Acc: 82.46%\n",
            "[ResNet-10 + BatchNorm] Epoch [56/100] Train Loss: 0.0002 | Test Loss: 1.0565 | Test Acc: 82.62%\n",
            "[ResNet-10 + BatchNorm] Epoch [57/100] Train Loss: 0.0002 | Test Loss: 1.0639 | Test Acc: 82.63%\n",
            "[ResNet-10 + BatchNorm] Epoch [58/100] Train Loss: 0.0002 | Test Loss: 1.0571 | Test Acc: 82.57%\n",
            "[ResNet-10 + BatchNorm] Epoch [59/100] Train Loss: 0.0002 | Test Loss: 1.0593 | Test Acc: 82.68%\n",
            "[ResNet-10 + BatchNorm] Epoch [60/100] Train Loss: 0.0002 | Test Loss: 1.0600 | Test Acc: 82.48%\n",
            "[ResNet-10 + BatchNorm] Epoch [61/100] Train Loss: 0.0002 | Test Loss: 1.0661 | Test Acc: 82.56%\n",
            "[ResNet-10 + BatchNorm] Epoch [62/100] Train Loss: 0.0002 | Test Loss: 1.0664 | Test Acc: 82.40%\n",
            "[ResNet-10 + BatchNorm] Epoch [63/100] Train Loss: 0.0002 | Test Loss: 1.0649 | Test Acc: 82.52%\n",
            "[ResNet-10 + BatchNorm] Epoch [64/100] Train Loss: 0.0002 | Test Loss: 1.0702 | Test Acc: 82.56%\n",
            "[ResNet-10 + BatchNorm] Epoch [65/100] Train Loss: 0.0002 | Test Loss: 1.0689 | Test Acc: 82.36%\n",
            "[ResNet-10 + BatchNorm] Epoch [66/100] Train Loss: 0.0001 | Test Loss: 1.0736 | Test Acc: 82.52%\n",
            "[ResNet-10 + BatchNorm] Epoch [67/100] Train Loss: 0.0001 | Test Loss: 1.0772 | Test Acc: 82.49%\n",
            "[ResNet-10 + BatchNorm] Epoch [68/100] Train Loss: 0.0001 | Test Loss: 1.0752 | Test Acc: 82.50%\n",
            "[ResNet-10 + BatchNorm] Epoch [69/100] Train Loss: 0.0001 | Test Loss: 1.0749 | Test Acc: 82.37%\n",
            "[ResNet-10 + BatchNorm] Epoch [70/100] Train Loss: 0.0001 | Test Loss: 1.0740 | Test Acc: 82.44%\n",
            "[ResNet-10 + BatchNorm] Epoch [71/100] Train Loss: 0.0001 | Test Loss: 1.0819 | Test Acc: 82.40%\n",
            "[ResNet-10 + BatchNorm] Epoch [72/100] Train Loss: 0.0001 | Test Loss: 1.0788 | Test Acc: 82.32%\n",
            "[ResNet-10 + BatchNorm] Epoch [73/100] Train Loss: 0.0001 | Test Loss: 1.0769 | Test Acc: 82.59%\n",
            "[ResNet-10 + BatchNorm] Epoch [74/100] Train Loss: 0.0001 | Test Loss: 1.0759 | Test Acc: 82.37%\n",
            "[ResNet-10 + BatchNorm] Epoch [75/100] Train Loss: 0.0001 | Test Loss: 1.0946 | Test Acc: 82.50%\n",
            "[ResNet-10 + BatchNorm] Epoch [76/100] Train Loss: 0.0001 | Test Loss: 1.0848 | Test Acc: 82.55%\n",
            "[ResNet-10 + BatchNorm] Epoch [77/100] Train Loss: 0.0001 | Test Loss: 1.0923 | Test Acc: 82.51%\n",
            "[ResNet-10 + BatchNorm] Epoch [78/100] Train Loss: 0.0001 | Test Loss: 1.0916 | Test Acc: 82.52%\n",
            "[ResNet-10 + BatchNorm] Epoch [79/100] Train Loss: 0.0001 | Test Loss: 1.0902 | Test Acc: 82.53%\n",
            "[ResNet-10 + BatchNorm] Epoch [80/100] Train Loss: 0.0001 | Test Loss: 1.0861 | Test Acc: 82.43%\n",
            "[ResNet-10 + BatchNorm] Epoch [81/100] Train Loss: 0.0001 | Test Loss: 1.1023 | Test Acc: 82.59%\n",
            "[ResNet-10 + BatchNorm] Epoch [82/100] Train Loss: 0.0001 | Test Loss: 1.0918 | Test Acc: 82.51%\n",
            "[ResNet-10 + BatchNorm] Epoch [83/100] Train Loss: 0.0001 | Test Loss: 1.0950 | Test Acc: 82.44%\n",
            "[ResNet-10 + BatchNorm] Epoch [84/100] Train Loss: 0.0001 | Test Loss: 1.0946 | Test Acc: 82.59%\n",
            "[ResNet-10 + BatchNorm] Epoch [85/100] Train Loss: 0.0001 | Test Loss: 1.0965 | Test Acc: 82.50%\n",
            "[ResNet-10 + BatchNorm] Epoch [86/100] Train Loss: 0.0001 | Test Loss: 1.0883 | Test Acc: 82.56%\n",
            "[ResNet-10 + BatchNorm] Epoch [87/100] Train Loss: 0.0001 | Test Loss: 1.0987 | Test Acc: 82.59%\n",
            "[ResNet-10 + BatchNorm] Epoch [88/100] Train Loss: 0.0001 | Test Loss: 1.1054 | Test Acc: 82.42%\n",
            "[ResNet-10 + BatchNorm] Epoch [89/100] Train Loss: 0.0001 | Test Loss: 1.1054 | Test Acc: 82.35%\n",
            "[ResNet-10 + BatchNorm] Epoch [90/100] Train Loss: 0.0001 | Test Loss: 1.1031 | Test Acc: 82.49%\n",
            "[ResNet-10 + BatchNorm] Epoch [91/100] Train Loss: 0.0001 | Test Loss: 1.1055 | Test Acc: 82.42%\n",
            "[ResNet-10 + BatchNorm] Epoch [92/100] Train Loss: 0.0001 | Test Loss: 1.1055 | Test Acc: 82.30%\n",
            "[ResNet-10 + BatchNorm] Epoch [93/100] Train Loss: 0.0001 | Test Loss: 1.1138 | Test Acc: 82.39%\n",
            "[ResNet-10 + BatchNorm] Epoch [94/100] Train Loss: 0.0001 | Test Loss: 1.1185 | Test Acc: 82.36%\n",
            "[ResNet-10 + BatchNorm] Epoch [95/100] Train Loss: 0.0001 | Test Loss: 1.1094 | Test Acc: 82.48%\n",
            "[ResNet-10 + BatchNorm] Epoch [96/100] Train Loss: 0.0001 | Test Loss: 1.1089 | Test Acc: 82.38%\n",
            "[ResNet-10 + BatchNorm] Epoch [97/100] Train Loss: 0.0001 | Test Loss: 1.1072 | Test Acc: 82.38%\n",
            "[ResNet-10 + BatchNorm] Epoch [98/100] Train Loss: 0.0001 | Test Loss: 1.1169 | Test Acc: 82.52%\n",
            "[ResNet-10 + BatchNorm] Epoch [99/100] Train Loss: 0.0001 | Test Loss: 1.1095 | Test Acc: 82.50%\n",
            "[ResNet-10 + BatchNorm] Epoch [100/100] Train Loss: 0.0001 | Test Loss: 1.1115 | Test Acc: 82.53%\n",
            "\n",
            "=== Final Results: ResNet-10 + BatchNorm ===\n",
            "Training time: 734.16 seconds\n",
            "Final training loss: 0.0001\n",
            "Final test loss: 1.1115\n",
            "Final test accuracy: 82.53%\n",
            "Model size (parameters): 346458\n",
            "\n",
            "===== Summary of All 3 Regularization Experiments =====\n",
            "ResNet-10 + Weight Decay (λ=0.001): time=767.54s, train_loss=0.0929, test_acc=81.49%, params=344634\n",
            "ResNet-10 + Dropout (p=0.3): time=761.95s, train_loss=0.0585, test_acc=79.69%, params=344634\n",
            "ResNet-10 + BatchNorm: time=734.16s, train_loss=0.0001, test_acc=82.53%, params=346458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyXUdx26pkW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}