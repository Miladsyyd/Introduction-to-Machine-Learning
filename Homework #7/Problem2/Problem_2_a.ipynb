{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xW0JUPVtlY8h"
      },
      "outputs": [],
      "source": [
        "# Mohammadmilad Sayyad\n",
        "# Problem 2.a\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size   = 128\n",
        "learning_rate = 0.01\n",
        "num_epochs    = 50    # <<< 50 epochs instead of 300\n",
        "\n",
        "# CIFAR-10 normalization\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std  = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQMAFYQal9qP",
        "outputId": "8bcc0680-b7bc-4a6e-d097-dccd71f47a37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5onE5JumAiB",
        "outputId": "8c9e87c2-0b61-4513-f655-0c111b34579d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 50000\n",
            "Test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Shortcut for match dim / stride\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels,\n",
        "                                      kernel_size=1, stride=stride, bias=False)\n",
        "        else:\n",
        "            self.shortcut = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        identity = x if self.shortcut is None else self.shortcut(x)\n",
        "        out = F.relu(out + identity)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "\n",
        "        self.in_channels = 16\n",
        "\n",
        "        # First conv (no pooling, CIFAR style)\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # 10 blocks total: 3 + 3 + 4\n",
        "        self.layer1 = self._make_layer(16, num_blocks=3, stride=1)  # 32x32\n",
        "        self.layer2 = self._make_layer(32, num_blocks=3, stride=2)  # 16x16\n",
        "        self.layer3 = self._make_layer(64, num_blocks=4, stride=2)  # 8x8\n",
        "\n",
        "        # Global average pooling + FC\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels, out_channels, stride=s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        # Global average pool to 1x1\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)  # (N, 64)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "model = ResNet10(num_classes=10).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgLG73XHmBmp",
        "outputId": "8bfbfa16-49db-4986-a6b3-389323f079cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet10(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_params = count_parameters(model)\n",
        "print(\"ResNet-10 trainable parameters:\", num_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbsrbkF3mC-R",
        "outputId": "dd95a672-3d1b-46e1-f53c-2a8d84d5eb63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet-10 trainable parameters: 344634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, criterion, dataloader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader.dataset)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "y0HVTOWVmFXA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Training ResNet-10 (Problem 2.a) for 50 epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f} | \"\n",
        "          f\"Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(\"\\n=== Final Results for Problem 2.a (ResNet-10, 50 epochs) ===\")\n",
        "print(f\"Training time: {training_time:.2f} seconds\")\n",
        "print(f\"Final training loss (epoch {num_epochs}): {train_losses[-1]:.4f}\")\n",
        "print(f\"Final test loss   (epoch {num_epochs}): {test_losses[-1]:.4f}\")\n",
        "print(f\"Final test accuracy (epoch {num_epochs}): {test_accuracies[-1]:.2f}%\")\n",
        "print(f\"Model size (parameters): {num_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCJTZPmdmG_R",
        "outputId": "173f9978-3c62-4aca-d5a5-39e9621fe901"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ResNet-10 (Problem 2.a) for 50 epochs...\n",
            "Epoch [1/50] Train Loss: 2.0672 | Test Loss: 1.8689 | Test Acc: 31.51%\n",
            "Epoch [2/50] Train Loss: 1.7330 | Test Loss: 1.6404 | Test Acc: 38.41%\n",
            "Epoch [3/50] Train Loss: 1.5500 | Test Loss: 1.4710 | Test Acc: 45.93%\n",
            "Epoch [4/50] Train Loss: 1.4009 | Test Loss: 1.2757 | Test Acc: 53.06%\n",
            "Epoch [5/50] Train Loss: 1.2742 | Test Loss: 1.3032 | Test Acc: 54.02%\n",
            "Epoch [6/50] Train Loss: 1.1432 | Test Loss: 1.1180 | Test Acc: 59.88%\n",
            "Epoch [7/50] Train Loss: 1.0489 | Test Loss: 1.1027 | Test Acc: 60.71%\n",
            "Epoch [8/50] Train Loss: 0.9485 | Test Loss: 0.9205 | Test Acc: 67.67%\n",
            "Epoch [9/50] Train Loss: 0.8674 | Test Loss: 0.8783 | Test Acc: 69.76%\n",
            "Epoch [10/50] Train Loss: 0.8014 | Test Loss: 0.8204 | Test Acc: 71.76%\n",
            "Epoch [11/50] Train Loss: 0.7394 | Test Loss: 0.7589 | Test Acc: 73.70%\n",
            "Epoch [12/50] Train Loss: 0.6672 | Test Loss: 0.7893 | Test Acc: 74.03%\n",
            "Epoch [13/50] Train Loss: 0.6307 | Test Loss: 0.6848 | Test Acc: 76.73%\n",
            "Epoch [14/50] Train Loss: 0.5851 | Test Loss: 0.7516 | Test Acc: 75.09%\n",
            "Epoch [15/50] Train Loss: 0.5421 | Test Loss: 0.6701 | Test Acc: 77.04%\n",
            "Epoch [16/50] Train Loss: 0.5038 | Test Loss: 0.6498 | Test Acc: 77.78%\n",
            "Epoch [17/50] Train Loss: 0.4647 | Test Loss: 0.6963 | Test Acc: 76.62%\n",
            "Epoch [18/50] Train Loss: 0.4356 | Test Loss: 0.6683 | Test Acc: 77.69%\n",
            "Epoch [19/50] Train Loss: 0.4039 | Test Loss: 0.6433 | Test Acc: 78.89%\n",
            "Epoch [20/50] Train Loss: 0.3782 | Test Loss: 0.6701 | Test Acc: 79.03%\n",
            "Epoch [21/50] Train Loss: 0.3520 | Test Loss: 0.6635 | Test Acc: 79.44%\n",
            "Epoch [22/50] Train Loss: 0.3178 | Test Loss: 0.6374 | Test Acc: 79.86%\n",
            "Epoch [23/50] Train Loss: 0.2909 | Test Loss: 0.6603 | Test Acc: 79.84%\n",
            "Epoch [24/50] Train Loss: 0.2772 | Test Loss: 0.7528 | Test Acc: 79.92%\n",
            "Epoch [25/50] Train Loss: 0.2438 | Test Loss: 0.8014 | Test Acc: 76.43%\n",
            "Epoch [26/50] Train Loss: 0.2265 | Test Loss: 0.7332 | Test Acc: 78.78%\n",
            "Epoch [27/50] Train Loss: 0.2072 | Test Loss: 0.7694 | Test Acc: 78.49%\n",
            "Epoch [28/50] Train Loss: 0.1956 | Test Loss: 0.8190 | Test Acc: 78.31%\n",
            "Epoch [29/50] Train Loss: 0.1775 | Test Loss: 0.9048 | Test Acc: 79.05%\n",
            "Epoch [30/50] Train Loss: 0.1623 | Test Loss: 0.8639 | Test Acc: 79.59%\n",
            "Epoch [31/50] Train Loss: 0.1563 | Test Loss: 0.8683 | Test Acc: 79.70%\n",
            "Epoch [32/50] Train Loss: 0.1398 | Test Loss: 0.8838 | Test Acc: 79.54%\n",
            "Epoch [33/50] Train Loss: 0.1449 | Test Loss: 0.9123 | Test Acc: 79.31%\n",
            "Epoch [34/50] Train Loss: 0.1281 | Test Loss: 0.9664 | Test Acc: 77.66%\n",
            "Epoch [35/50] Train Loss: 0.1193 | Test Loss: 1.0031 | Test Acc: 78.76%\n",
            "Epoch [36/50] Train Loss: 0.1189 | Test Loss: 0.9491 | Test Acc: 79.53%\n",
            "Epoch [37/50] Train Loss: 0.1097 | Test Loss: 0.9462 | Test Acc: 79.95%\n",
            "Epoch [38/50] Train Loss: 0.1008 | Test Loss: 1.0557 | Test Acc: 79.24%\n",
            "Epoch [39/50] Train Loss: 0.1066 | Test Loss: 0.9653 | Test Acc: 79.73%\n",
            "Epoch [40/50] Train Loss: 0.0849 | Test Loss: 1.0214 | Test Acc: 79.72%\n",
            "Epoch [41/50] Train Loss: 0.0848 | Test Loss: 1.1615 | Test Acc: 78.74%\n",
            "Epoch [42/50] Train Loss: 0.0861 | Test Loss: 1.1077 | Test Acc: 79.08%\n",
            "Epoch [43/50] Train Loss: 0.0789 | Test Loss: 1.0570 | Test Acc: 79.07%\n",
            "Epoch [44/50] Train Loss: 0.0827 | Test Loss: 1.0336 | Test Acc: 79.97%\n",
            "Epoch [45/50] Train Loss: 0.0775 | Test Loss: 1.1410 | Test Acc: 79.83%\n",
            "Epoch [46/50] Train Loss: 0.0636 | Test Loss: 1.4240 | Test Acc: 78.85%\n",
            "Epoch [47/50] Train Loss: 0.0698 | Test Loss: 1.1184 | Test Acc: 78.99%\n",
            "Epoch [48/50] Train Loss: 0.0792 | Test Loss: 1.1093 | Test Acc: 79.50%\n",
            "Epoch [49/50] Train Loss: 0.0591 | Test Loss: 1.2996 | Test Acc: 78.11%\n",
            "Epoch [50/50] Train Loss: 0.0671 | Test Loss: 1.1763 | Test Acc: 79.55%\n",
            "\n",
            "=== Final Results for Problem 2.a (ResNet-10, 50 epochs) ===\n",
            "Training time: 381.19 seconds\n",
            "Final training loss (epoch 50): 0.0671\n",
            "Final test loss   (epoch 50): 1.1763\n",
            "Final test accuracy (epoch 50): 79.55%\n",
            "Model size (parameters): 344634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0r1cOZ9mJUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}